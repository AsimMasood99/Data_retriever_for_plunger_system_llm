{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# Ensure local project directory is prioritized\n",
    "sys.path.insert(0, os.path.abspath(\"injections\"))\n",
    "\n",
    "from Plunger_helper import raw_json\n",
    "# import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())  \n",
    "# --- LLM Setup ---\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def safe_format(template, **kwargs):\n",
    "    return template.format_map(defaultdict(lambda: \"\", kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def clean_markdown_json(text):\n",
    "    # Remove triple backticks + optional language tag\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n\", \"\", text)\n",
    "    cleaned = re.sub(r\"\\n```$\", \"\", cleaned)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from fastapi) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/smirfan/miniconda3/envs/MLDL/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: starlette, fastapi\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [fastapi]m1/2\u001b[0m [fastapi]\n",
      "\u001b[1A\u001b[2KSuccessfully installed fastapi-0.116.1 starlette-0.47.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decomposing user query...\n",
      "\n",
      "Decomposed Query:\n",
      " **Query Breakdown**\n",
      "\n",
      "* **Objective**: Identify a specific cycle\n",
      "* **Condition**: Plunger did not arrive\n",
      "* **Metrics/Filters**: \n",
      "\t+ Item: Plunger\n",
      "\t+ Status: Did not arrive\n",
      "* **Expected Result**: A specific cycle where the plunger did not arrive\n",
      "\n",
      " user_query show me the cycle where plunger did not arrive\n",
      "\n",
      "Sending SQL generation prompt to LLM...\n",
      "\n",
      "SQL Generated:\n",
      " SELECT e.cycle_id \n",
      "FROM EVENTS e \n",
      "JOIN PLUNGER_ARRIVAL_STATUS_EVENTS p \n",
      "ON e.plunger_arrival_status_event = p.rowid \n",
      "WHERE p.non_arrival = 1;\n",
      "\n",
      " Sending SQL to server:\n",
      "SELECT e.cycle_id \n",
      "FROM EVENTS e \n",
      "JOIN PLUNGER_ARRIVAL_STATUS_EVENTS p \n",
      "ON e.plunger_arrival_status_event = p.rowid \n",
      "WHERE p.non_arrival = 1;\n",
      " Raw Response: 200 [{\"cycle_id\": 121}, {\"cycle_id\": 217}, {\"cycle_id\": 218}, {\"cycle_id\": 458}, {\"cycle_id\": 459}, {\"cycle_id\": 461}, {\"cycle_id\": 462}, {\"cycle_id\": 468}, {\"cycle_id\": 469}, {\"cycle_id\": 121}, {\"cycle_id\": 217}, {\"cycle_id\": 218}, {\"cycle_id\": 458}, {\"cycle_id\": 459}, {\"cycle_id\": 461}, {\"cycle_id\": 462}, {\"cycle_id\": 468}, {\"cycle_id\": 469}]\n",
      "\n",
      "Raw Query Result:\n",
      "[{'cycle_id': 121},\n",
      " {'cycle_id': 217},\n",
      " {'cycle_id': 218},\n",
      " {'cycle_id': 458},\n",
      " {'cycle_id': 459},\n",
      " {'cycle_id': 461},\n",
      " {'cycle_id': 462},\n",
      " {'cycle_id': 468},\n",
      " {'cycle_id': 469},\n",
      " {'cycle_id': 121},\n",
      " {'cycle_id': 217},\n",
      " {'cycle_id': 218},\n",
      " {'cycle_id': 458},\n",
      " {'cycle_id': 459},\n",
      " {'cycle_id': 461},\n",
      " {'cycle_id': 462},\n",
      " {'cycle_id': 468},\n",
      " {'cycle_id': 469}]\n",
      "\n",
      "Generating human-friendly summary...\n",
      "\n",
      "Final Answer:\n",
      " **Summary: Cycles with No Plunger Arrival**\n",
      "\n",
      "We've identified the cycles where the plunger did not arrive as requested. After analyzing the data, we found that the following cycles were affected: 121, 217, 218, 458, 459, 461, 462, 468, and 469.\n",
      "\n",
      "In simpler terms, this means that during these specific cycles, the plunger was expected to arrive but did not. This information can help you investigate the reasons behind these incidents and take corrective actions to prevent them in the future.\n",
      "\n",
      "Note that some of these cycle IDs appeared multiple times in the results, but we've removed the duplicates to provide a clear and concise list. If you have any further questions or would like to explore this data in more detail, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "def get_chat_completion(prompt, model) -> str:\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "\n",
    "def execute_sql(sql: str) -> dict:\n",
    "    print(\"\\n Sending SQL to server:\")\n",
    "    print(sql)\n",
    "    \n",
    "    response = requests.post(\"http://localhost:8765/\", data=sql)\n",
    "    print(\" Raw Response:\", response.status_code, response.text)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    user_query = input(\"Enter your plunger lift question: \")\n",
    "    \n",
    "    event_context = raw_json[\"PromptInjection\"]\n",
    "\n",
    "    decomposer_prompt = f\"\"\"\n",
    "You are a task decomposer. Break down the user's natural language question into a structured query that highlights what the user wants to know, what metrics or filters are involved, and what kind of result is expected.\n",
    "\n",
    "## User Query:\n",
    "{user_query}\n",
    "\n",
    "Return a structured breakdown only.\n",
    "\"\"\".strip()\n",
    "\n",
    "    print(\"\\nDecomposing user query...\")\n",
    "    structured_query = get_chat_completion(decomposer_prompt, llm)\n",
    "    print(\"\\nDecomposed Query:\\n\", structured_query.strip())\n",
    "    print(\"\\n user_query\", user_query)\n",
    "    sql_prompt = f\"\"\"\n",
    "You are an expert SQL generator. Given a structured query and schema context, return a valid SQL query for an SQLite database. Do not explain anything.\n",
    "\n",
    "## Structured Query:\n",
    "{structured_query}\n",
    "\n",
    "## Schema and Event Hierarchy:\n",
    "{event_context}\n",
    "\n",
    "ONLY return the SQL query.\n",
    "\"\"\".strip()\n",
    "\n",
    "    print(\"\\nSending SQL generation prompt to LLM...\")\n",
    "    sql = get_chat_completion(sql_prompt, llm)\n",
    "\n",
    "    sql = sql.strip().strip(\"```sql\").strip(\"```\").strip()\n",
    "    sql = sql.rstrip(\";\") + \";\"\n",
    "\n",
    "    print(\"\\nSQL Generated:\\n\", sql)\n",
    "\n",
    "    try:\n",
    "        result = execute_sql(sql)\n",
    "        print(\"\\nRaw Query Result:\")\n",
    "        from pprint import pprint\n",
    "        pprint(result)\n",
    "    except Exception as e:\n",
    "        print(\"\\nSQL execution failed:\", str(e))\n",
    "        return\n",
    "\n",
    "    summary_prompt = f\"\"\"\n",
    "You are a data analyst. Based on the following SQL query, its result, and the user’s original question, generate a clear and concise explanation that helps a human understand the insight in plain English.\n",
    "\n",
    "## User Query:\n",
    "{user_query}\n",
    "\n",
    "## SQL Query:\n",
    "{sql}\n",
    "\n",
    "## Query Result:\n",
    "{result}\n",
    "\n",
    "Write a friendly, human-readable summary.\n",
    "\"\"\".strip()\n",
    "\n",
    "    print(\"\\nGenerating human-friendly summary...\")\n",
    "    final_summary = get_chat_completion(summary_prompt, llm)\n",
    "    print(\"\\nFinal Answer:\\n\", final_summary.strip())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_chat_completion(prompt, model) -> str:\n",
    "#     response = model.invoke(prompt)\n",
    "#     return response.content.strip()\n",
    "\n",
    "# def execute_sql(sql: str) -> dict:\n",
    "#     print(\"\\n Sending SQL to server:\")\n",
    "#     print(sql)\n",
    "    \n",
    "#     response = requests.post(\"http://localhost:8765/\", data=sql)\n",
    "#     print(\" Raw Response:\", response.status_code, response.text)\n",
    "#     response.raise_for_status()\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# def load_prompt(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return f.read().strip()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     user_query = input(\"Enter your plunger lift question: \")\n",
    "\n",
    "#     # Example: non-nested key access with error handling\n",
    "#     event_context1 = raw_json.get(\"schema\", \"Default schema value or handle missing key\")\n",
    "#     event_context2 = raw_json.get(\"PromptInjection1\", \"Default PromptInjection1 value or handle missing key\")\n",
    "#     event_context3 = raw_json.get(\"event_details\", \"Default event_details value or handle missing key\")\n",
    "\n",
    "#     # Load prompt templates\n",
    "#     decomposer_template = load_prompt(\"prompts/decomposer.md\")\n",
    "#     sql_template = load_prompt(\"prompts/sqlquery.md\")\n",
    "#     summary_template = load_prompt(\"prompts/final_analyst.md\")\n",
    "\n",
    "#     # Format decomposer prompt\n",
    "#     decomposer_prompt = decomposer_template.format(user_query=user_query)\n",
    "\n",
    "#     print(\"\\nDecomposing user query...\")\n",
    "#     structured_query = get_chat_completion(decomposer_prompt, llm)\n",
    "#     print(\"\\nDecomposed Query:\\n\", structured_query.strip())\n",
    "\n",
    "#     # Format SQL generation prompt\n",
    "#     sql_prompt = safe_format(\n",
    "#         sql_template,\n",
    "#         structured_query=structured_query.strip(),\n",
    "#         schema=event_context1,\n",
    "#         PromptInjection1=event_context2,\n",
    "#         event_details=event_context3\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"\\nSending SQL generation prompt to LLM...\")\n",
    "#     sql = get_chat_completion(sql_prompt, llm)\n",
    "\n",
    "#     sql = sql.strip().strip(\"```sql\").strip(\"```\").strip()\n",
    "#     sql = sql.rstrip(\";\") + \";\"\n",
    "\n",
    "#     print(\"\\nSQL Generated:\\n\", sql)\n",
    "\n",
    "#     try:\n",
    "#         result = execute_sql(sql)\n",
    "#         print(\"\\nRaw Query Result:\")\n",
    "#         from pprint import pprint\n",
    "#         pprint(result)\n",
    "#     except Exception as e:\n",
    "#         print(\"\\nSQL execution failed:\", str(e))\n",
    "#         return\n",
    "\n",
    "#     # Format summary prompt\n",
    "#     summary_prompt = summary_template.format(\n",
    "#         user_query=user_query,\n",
    "#         sql=sql,\n",
    "#         result=result\n",
    "#     )\n",
    "\n",
    "#     print(\"\\nGenerating human-friendly summary...\")\n",
    "#     final_summary = get_chat_completion(summary_prompt, llm)\n",
    "#     print(\"\\nFinal Answer:\\n\", final_summary.strip())\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
