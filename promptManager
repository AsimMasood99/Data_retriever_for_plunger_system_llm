import os
import json
import sqlite3
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import JsonOutputParser, StrOutputParser

# --- 1. SETUP: Load environment and initialize LLM ---
load_dotenv()
llm = ChatGroq(model_name="llama3-8b-8192", temperature=0)

# --- 2. LANGCHAIN CHAIN DEFINITIONS ---

def get_decomposer_chain():
    """Returns a chain that deconstructs a user query into JSON scenarios."""
    return ChatPromptTemplate.from_template_file("prompts/decomposer.md") | llm | JsonOutputParser()

def get_sql_generator_chain():
    """Returns a chain that generates a SQL query string from a description."""
    return ChatPromptTemplate.from_template_file("prompts/query.md") | llm | StrOutputParser()

def get_analyst_chain():
    """Returns a chain that synthesizes data into a final natural language summary."""
    return ChatPromptTemplate.from_template_file("prompts/analyst.md") | llm | StrOutputParser()

# --- 3. DATABASE HELPER FUNCTION ---

def execute_sql_query(sql_query):
    """Placeholder function to simulate database execution."""
    print(f"--- DATABASE INTERACTION (SKIPPED) --- Would execute: {sql_query}")
    if "UNSAFE_VELOCITY_EVENTS" in sql_query:
        return [{"event_id": "uv-001", "arrival_speed": 15.2, "timestamp": "2024-08-06 14:30:00"}]
    if "LOW_FLOW_CYCLES" in sql_query:
        return [{"average_pressure": 465.3}]
    return [{"error": "Query execution is currently disabled."}]

# --- 4. MAIN WORKFLOW FUNCTION ---

def run_query_pipeline(user_query, db_schema):
    """
    Executes the full pipeline from user query to a final, synthesized natural language answer.
    """
    print("--- Starting Query Pipeline ---")

    # Load static context data needed for prompts
    with open('injections/injection.json', 'r') as f:
        injection_data = json.load(f)
    event_details = injection_data.get("event_details", "Event details context not found.")

    # Get the initialized LangChain chains
    decomposer_chain = get_decomposer_chain()
    sql_generator_chain = get_sql_generator_chain()
    analyst_chain = get_analyst_chain()

    # STEP 1: Decompose the user query
    print("\n[1] Decomposing user query...")
    scenarios = decomposer_chain.invoke({"query": user_query})

    # STEP 2 & 3: Generate SQL and fetch data for each scenario
    print("\n[2] Generating SQL and fetching data...")
    retrieved_data = []
    for description in scenarios.values():
        generator_input = {"schema": db_schema, "event_details": event_details, "description": description}
        sql_query = sql_generator_chain.invoke(generator_input)
        query_result = execute_sql_query(sql_query)
        retrieved_data.append({"description": description, "sql_result": query_result})

    # STEP 4: Format the retrieved data and synthesize the final answer
    print("\n[3] Synthesizing final answer...")
    
    # Format the retrieved data into a structured markdown string for the analyst prompt
    retrieved_data_summary_str = ""
    for i, item in enumerate(retrieved_data, 1):
        description = item['description']
        sql_result_str = json.dumps(item['sql_result'], indent=2)
        retrieved_data_summary_str += (
            f"**Scenario {i}: {description}**\n"
            f"*Data Found:*\n"
            f"```json\n{sql_result_str}\n```\n---\n"
        )

    analyst_input = {
        "original_query": user_query,
        "retrieved_data_summary": retrieved_data_summary_str,
        "event_details": event_details
    }
    
    final_summary = analyst_chain.invoke(analyst_input)

    print("\n--- Pipeline Finished ---")
    return final_summary

# --- EXAMPLE OF HOW TO CALL THE FUNCTION ---

# my_user_query = "Find all unsafe velocity events from the last day, and what was the average casing pressure during low flow cycles yesterday?"
# my_db_schema = """
# CREATE TABLE UNSAFE_VELOCITY_EVENTS (event_id TEXT, timestamp TEXT, arrival_speed REAL);
# CREATE TABLE LOW_FLOW_CYCLES (cycle_id TEXT, timestamp TEXT, avg_casing_pressure REAL);
# """

# # Call the main workflow function to get the final, human-readable answer
# final_answer = run_query_pipeline(my_user_query, my_db_schema)

# # Print the result
# print("\n--- FINAL ANALYST RESPONSE ---")
# print(final_answer)