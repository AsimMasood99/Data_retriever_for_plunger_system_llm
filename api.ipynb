{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# Ensure local project directory is prioritized\n",
    "sys.path.insert(0, os.path.abspath(\"injections\"))\n",
    "\n",
    "from Plunger_helper import raw_json\n",
    "# import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())  \n",
    "# --- LLM Setup ---\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def safe_format(template, **kwargs):\n",
    "    return template.format_map(defaultdict(lambda: \"\", kwargs))\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "def clean_markdown_json(text):\n",
    "    # Remove triple backticks + optional language tag\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n\", \"\", text)\n",
    "    cleaned = re.sub(r\"\\n```$\", \"\", cleaned)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import requests\n",
    "\n",
    "# Your existing functions\n",
    "def get_chat_completion(prompt, model) -> str:\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "\n",
    "def execute_sql(sql: str) -> dict:\n",
    "    print(\"\\n Sending SQL to server:\")\n",
    "    print(sql)\n",
    "    \n",
    "    response = requests.post(\"http://localhost:8765/\", data=sql)\n",
    "    print(\" Raw Response:\", response.status_code, response.text)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Allow frontend calls\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Change to frontend URL in prod\n",
    "    allow_methods=[\"POST\"],\n",
    "    allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "# Request model\n",
    "class ChatRequest(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(req: ChatRequest):\n",
    "    user_query = req.user_query\n",
    "    event_context = raw_json[\"PromptInjection\"]\n",
    "\n",
    "    # Decompose query\n",
    "    decomposer_prompt = f\"\"\"\n",
    "You are a task decomposer. Break down the user's natural language question into a structured query that highlights what the user wants to know, what metrics or filters are involved, and what kind of result is expected.\n",
    "## User Query:\n",
    "{user_query}\n",
    "Return a structured breakdown only.\n",
    "\"\"\".strip()\n",
    "    structured_query = get_chat_completion(decomposer_prompt, llm)\n",
    "\n",
    "    # Generate SQL\n",
    "    sql_prompt = f\"\"\"\n",
    "You are an expert SQL generator. Given a structured query and schema context, return a valid SQL query for an SQLite database. Do not explain anything.\n",
    "\n",
    "## Structured Query:\n",
    "{structured_query}\n",
    "## Schema and Event Hierarchy:\n",
    "{event_context}\n",
    "ONLY return the SQL query.\n",
    "\n",
    "\"\"\".strip()\n",
    "    print(\"\\nSending SQL generation prompt to LLM...\")\n",
    "    sql = get_chat_completion(sql_prompt, llm).strip().strip(\"```sql\").strip(\"```\").strip()\n",
    "    sql = sql.rstrip(\";\") + \";\"\n",
    "\n",
    "    print(\"\\nSQL Generated:\\n\", sql)\n",
    "\n",
    "    # Execute SQL\n",
    "    try:\n",
    "        result = execute_sql(sql)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    # Generate summary\n",
    "    summary_prompt = f\"\"\"\n",
    "You are a data analyst. Based on the following SQL query, its result, and the userâ€™s original question, generate a clear and concise explanation that helps a human understand the insight in plain English.\n",
    "\n",
    "## User Query:\n",
    "{user_query}\n",
    "## SQL Query:\n",
    "{sql}\n",
    "## Query Result:\n",
    "{result}\n",
    "Write a friendly, human-readable summary.\n",
    "\"\"\".strip()\n",
    "    final_summary = get_chat_completion(summary_prompt, llm)\n",
    "\n",
    "    return {\n",
    "        \"structured_query\": structured_query,\n",
    "        \"sql\": sql,\n",
    "        \"result\": result,\n",
    "        \"summary\": final_summary\n",
    "    }\n",
    "\n",
    "# Run: uvicorn api:app --reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
